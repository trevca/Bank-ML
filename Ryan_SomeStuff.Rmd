---
title: "STA141 Bank Project"
author: "Ryan Buchner, Trevor Carpenter, Billy Chow"
date: "12/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, echo = FALSE}
library(tidyverse)
library(randomForest)
library(missForest)
library(PCAmixdata)
library(gbm)
library(pROC)
library(ROSE)
library(Hmisc)
library(GPfit)
library(lintools)
library(gridExtra)
```

```{r, echo = FALSE}
# For Reproducability
set.seed(1)

```

# Background  
  
  A Portuguese banking institution has provided data related to a direct marketing campaign conducted through phone calls. Through the campaign, the institution collected information about the attributes of the contacted clients such as job type, marital status, and education level. The objective of the campaign was to use the collected data to assess whether or not a client would subscribe to a bank term deposit with the banking institution. The collected data consists of two categories of information, information related to the client and information related to the clients interactions with the marketing campaign. Information related to the client refers to attributes specific to the particular client, such as their age, job, or loan status. Information related to the clients interactions with the campaign refers to information such as the last date of contact with the campaign, method of contact, and the outcome of contact with previous marketing campaigns.
  
# Statistical Questions of Interest

Our main question of interest is how the prediction variables interact to predict whether or not a client will sign on to a long-term deposit. This entails building a predictive model to make educated guesses about if a particular individual will sign up for the loan. However, given that the data consists of both information about the person, we would like to see how the profile of an “average” person who signs up changes in different economic scenarios.

On top of that, we are also curious to see how much predictive power the extra data provides. That is, we would like to see how good of a model we can create without that extra data.

Finally, the dataset contains lots of missing data; we would like to see how the removal of this data before any preprocessing would affect the data.

# Data Analysis  

  One key thing to note about our dataset is that along with the response variable “y,” there is also the “duration of call” variable which we would not know before we called the individual. Thus, assuming this model is going to be used to select who to call, we are choosing to remove this. This will very much hurt our model’s power, but from the perspective of the bank it is necessary to do.

  Given our data set is overpopulated by people who didn’t sign up for the program, using a metric of proportion correctly classified doesn’t make sense, since a model that predicts “no” for every individual will perform very well, resulting in the models we build being skewed to predicting “no”. The bank is presumably most interested in identifying the people who sign up, so we need a better metric for evaluating our model. Thus, we will use a receiver operating characteristic (ROC) curve, which plots the specificity of the model against the sensitivity. The area under the  ROC curve (AUC) provides a metric of how well the model can pick up on True-positives while not substantially increasing the number of false positives. One technique we can use to help us increase the model’s predictive power is to undersample. This will remove some of the “no” samples from our training set; in effect this increases the weighting of the "yes" samples, thus effectively increasing the penalty for misclassifying them. Doing so will increase the AUC of the model, since th

### Data Processing
```{r, echo = FALSE}
# Read in data with specifications
bank_data=read_delim("bank-additional/bank-additional-full.csv",";",col_names=TRUE,col_types = "dfffffffcfddddfdddddf")

```


```{r, echo = FALSE}
# Replace unknowns with NA
bank_data[bank_data=="unknown"]=NA

```

```{r, echo = FALSE}
# Change the response variable to numeric.
bank_data$y=ifelse(bank_data$y=="yes",1,0)

```


##### Duration of Call 
We remove "call duration" since technically we wouldn't know this before we called the person up. However, this variable is a very strong predictor of sucess so removing it will hurt our model.
```{r, echo = FALSE}
bank_data = within(bank_data,rm("duration"))
```

##### Education Level  
Education has a logical order to it so we can convert it to a numeric. This works for trees, but not necessarily for neural networks or regression since the distances between the levels which we made all equal to 1 are arbitrary. We will mean impute for the missing values, assuming that they are average in terms of education. This is an assumption we are making and could be false (e.g. unknown educaton people might actually be less educated on average)
```{r, echo = FALSE}
bank_data$education=ifelse(bank_data$education=="illiterate",0,ifelse(bank_data$education=="basic.4y",1,ifelse(bank_data$education== "basic.6y",2,ifelse(bank_data$education== "basic.9y",3,ifelse(bank_data$education== "high.school",4,ifelse(bank_data$education== "university.degree",5,ifelse(bank_data$education== "professional.course",0,NA)))))))
bank_data$education=as.numeric(bank_data$education)
mean=mean(bank_data$education,na.rm=TRUE) 
bank_data$education[which(is.na(bank_data$education))]=mean
```

##### Job of Client 
We leave "unknown" as a job category. It is a large enough group on its own and we would lose information by imputing.
```{r, echo = FALSE}
ggplot(bank_data,aes(job,fill=as.factor(y)))+geom_bar()
bank_data$housing[which(is.na(bank_data$housing))]="unknown"
```

There appears to be a correlation between job and education, but it doesn't appear strong enough to justify removing one. And as we will see from our model, neither end up as super important variables anyways.
```{r, echo = FALSE}
names=c()
means=c()
for (i in 1:12){
  job=as.character(unique(bank_data$job)[i])
  names=c(names,job)
  means=c(means,mean(bank_data[which(bank_data$job==job),]$education))
}
barplot(means~names,data=data.frame(names=names,means=means),las=2,xlab="",ylab="Mean Education Level")
rm(job)
rm(names)
rm(means)
rm(i)
```


##### Has Credit in Default?
Default has only 3 occurences of "yes" so it won't be a good predictor. If ever used, it could cause overfitting of "yes" individuals. Might want to reconsider once looking at fill data set since that might have more "yes" occurences to the point where we can reasonably include it.
```{r, echo = FALSE}
cat("Number of 'yes' default values: ", length(which(bank_data$default=="yes")), "\n")
bank_data=within(bank_data,rm("default"))
```


##### Economic Factors  
Analyze the economic variables. "nr.employed" is number of people employed, "emp.var.rate" is a measure of economic volatability, "euribor3m" is a measure of interest rates, "cons.price.idx" (consumer price index) is a measure of the change in prices for consumer goods and services, and "cons.conf.idx" (consumer confidence index) measures optimism about the economy.
```{r, echo = FALSE}
econ=data.frame(emp=bank_data$nr.employed,evr=bank_data$emp.var.rate,eur=bank_data$euribor3m,pri=bank_data$cons.price.idx ,conf=bank_data$cons.conf.idx)
heatmap(cor(econ),scale="none")
rm(econ)
```

There is too much correlation between the nr.employed, emp.var.rate, and euribor3m, so we will remove two of them. Since they all provide essentially the same information, removing them will force the model to make predictions off other variables as well instead of relying on these variable exclusively.
```{r, echo = FALSE}
bank_data=within(bank_data,rm("nr.employed"))
bank_data=within(bank_data,rm("emp.var.rate"))
```

##### Outcome of Previous Contact  
Even though p-outcome is dominated by "non-existent," it seems that there are enough of yes/no in each of the categories to avoid overfitting. Thus, we can avoid making any changes.
```{r, echo = FALSE}
ggplot(bank_data,aes(x=poutcome,fill=as.factor(y)))+geom_bar()
```

##### Day of the Week  
There is some order to day of the week, so we changed it to a numeric variable, and then mean imputed to fill in missing data. However, we tested using this as a categorical variable as well and it appeared to make no difference, most likely because it is not a strong predictor.
```{r, echo = FALSE}
bank_data$day_of_week=ifelse(bank_data$day_of_week=="mon",0,ifelse(bank_data$day_of_week=="tue",1,ifelse(bank_data$day_of_week=="wed",2,ifelse(bank_data$day_of_week=="thu",3,ifelse(bank_data$day_of_week== "fri",4,NA)))))
bank_data$day_of_week=as.numeric(bank_data$day_of_week)
bank_data$day_of_week=as.numeric(bank_data$day_of_week)
mean=mean(bank_data$day_of_week,na.rm=TRUE)
bank_data$day_of_week[which(is.na(bank_data$day_of_week))]=mean#mean imputation
```

##### Has Housing Loan  
Housing is equally distributed between yes and no. No clear way to proceed with the missing values here through mode imputation, so it seems the best option will be to leave unknown as its own category.
```{r, echo = FALSE}
ggplot(bank_data,aes(x=housing,fill=as.factor(y)))+geom_bar()
bank_data$housing[which(is.na(bank_data$housing))]="unknown"
```

Will be best to leave unknown as its own category for "loan". Enough data points where we can justify leaving it as its own category.
```{r, echo = FALSE}
ggplot(bank_data,aes(x=loan,fill=as.factor(y)))+geom_bar()
bank_data$loan[which(is.na(bank_data$loan))]="unknown"
cat("Number of unknown loan values:", length(which(bank_data$loan=="unknown")), "\n")
```

##### Martital Status  
Good distribution of data for the marital category, but very few "unknowns" to the point where leaving them as their own category could cause overfitting. Instead we propose mode imputation.
```{r, echo = FALSE}
ggplot(bank_data,aes(x=marital,fill=as.factor(y)))+geom_bar()
cat("Number of unknown marital values", length(which(is.na(bank_data$marital))), "\n")
bank_data$marital=impute(bank_data$marital,fun=mode)
```

##### Number of Days since last contacted  
To avoid overfitting, we will make this categorical with 2 categories,<999 and >=999. This is because there are two strong groups, but if we look at the individual counts of the data, the different values at the lower end of the spectrum all have low individual frequency. This does have a strong correaltion with poutcome though, so we will choose to remove poutcome.
```{r, echo = FALSE, warning = FALSE}
ggplot(bank_data,aes(x=pdays,fill=as.factor(y)))+geom_histogram()
ggplot(bank_data[which(bank_data$pdays<900),],aes(x=pdays,fill=as.factor(y)))+geom_histogram()
bank_data$pdays=as.factor(ifelse(bank_data$pdays<999,"recent","long_ago"))
ggplot(bank_data,aes(x=pdays,fill=as.factor(y)))+geom_bar()
ggplot(bank_data,aes(x=pdays,fill=poutcome))+geom_bar()
bank_data=within(bank_data,rm("poutcome"))
```

##### Method of Contact  
There is nothing we expect to be correlated with contact, so no filtering to do here.
```{r, echo = FALSE}
ggplot(bank_data,aes(x=contact,fill=as.factor(y)))+geom_bar()
```

##### Month  
Month data seems alright, except some of the months lack data points, which could lead to overfitting. As a result, we will assign all December cases to a joint Nov/Dec variable.
```{r, echo = FALSE}
ggplot(bank_data,aes(x=month,fill=as.factor(y)))+geom_bar()
cat("Number of December month values:", length(which(bank_data$month=="dec" & bank_data$y==0)), "\n")
bank_data$month[which(bank_data$month=="nov" | bank_data$month=="dec")]="nov/dev"
ggplot(bank_data,aes(x=month,fill=as.factor(y)))+geom_bar()
bank_data$month=as.factor(bank_data$month)
```

##### Number of Previous Contacts  
There are not enough 5's, and 6's. We will merge those 2 together to create a more robust model.
```{r, echo = FALSE}
ggplot(bank_data,aes(x=previous,fill=as.factor(y)))+geom_bar()
bank_data$previous[which(bank_data$previous>=5)]=5
```

##### Client Age
Look at age distribution, may want to remove outliers since they may lead to overfitting for those points in the test set. However, through cross validation we realized that removing these data points hurts the models efficiency.
```{r, echo = FALSE, warning=FALSE}
quantile(bank_data$age)
ggplot(bank_data,aes(age,fill=as.factor(y)))+geom_histogram()
```

# Literature review  

Gradient boosting machines were first introduced in 1999 as a method of greedy function approximation; despite our usage for our project, the theory can be applied to other functional approximations methods besides trees. The methodology involves first fitting a simple function to the data, and then proceeding to successively fit models to the residual of the previous function, and additively combining the previous function with the new one [1]. More recently, improvements have been made to the base theory, but those improvements, such as xgboost, center around the speed of calculation, in particular allowing the training to occur in parallel [2].
Hyperparameter tuning has been known to be an important part of creating effective machine learning models for decades. Initially, hyperparameter tuning was done by hand, but more recently work has been done to automate the process. Bayesian optimization is by no means a new development, but its widespread adoption by the machine learning community did not occur until the past decade. In a 2012 paper, Snoek, Larochelle, and Adams, describe a process for Bayesian optimization in Machine Learning and tout its ability to create better predictive models. The process is a way to fit a function to the models “score” (which can be defined based on the situation) over the different hyperparameters; it is an iterative process which involves fitting a functions for both the mean and variance of the function, and then choosing a new point to sample based on the means and variances of the different regions [3]. 

# Extra Credit Methods 

In our model, since we chose to use boosting, we introduce a hyperparameter, the shrinkage of the boosting model. While exhaustive search methods (random search, graph search) perform adequately well, we chose to use Bayesian Optimization to find an ideal combination of hyperparameters. This method gives a statistical backing to our search, and offers better performance than the exhaustive methods in cases such as ours where there is a non-negligible cost of sampling from the distribution. Sampling for us involves training a boosted tree on the data for each of the k-fold cross validation sets and evaluating the average performance on the held out data points. This takes a significant amount of time. Instead of choosing points randomly, Bayesian Optimization works by first creating a Gaussian posterior function for the mean and covariance over the points of interest. That is, for the range specified, we will estimate the mu for all of the points as well as its  From this, we then select a new point to sample by looking for a point with high expected performance from a region that has high uncertainty (variance). This can be described as balancing the exploration- looking at areas with high variance, “unknown areas”- and exploitation- sampling from high performing areas to better find the absolute peak. While there are several different methods for selecting the new point, we chose to use “Expected Improvement” metric.

In the demonstration below, we plotted the posterior distribution for the mu side by side with the Expected Improvement function. For each iteration, we then sample the point specified by the maximum of Expected Improvement. We then re-calculate the posterior as well as the EI. This new pot is plotted below the original plots. We continue this for 10 interactions. Initially, we chose 5 points equally spaced out. Notice how the algorithm switches over from exploration- sampling points with high variance- to exploitation- sampling from points close to the maximum.

All of the code for the Bayesian Optimization was written by our team using only base R functions which took a significant amount of time. As a result we believe that we should be eligible to earn 15 extra credit points for this.

# Our Models

### Question 1: How can we build our model in order to best assist the bank in predicting users who are likely to sign up for a deposit.

The most simplistic model we could create for this problem would be a simple random forest model that best predicts based on our training and testing data.
```{r, echo = FALSE}
# Question 1
rf_bank_data = na.exclude(bank_data)
rf_bank_data$y = factor(rf_bank_data$y)
n=nrow(rf_bank_data)
ntree=1500 #Number of Trees
tsize=5000 #size of test set
sam=sample(n,tsize)
test=rf_bank_data[sam,]
train=rf_bank_data[-sam,]

rf = randomForest(y ~ ., train)
pred = predict(rf, test)
pred = ifelse(pred == "yes", 1, 0)
test$y = ifelse(test$y=="yes", 1, 0)
cat("Accuracy:", mean(pred == test$y), "\n")
table(pred, test$y)
```

However, clearly the initial struggle with a simplistic random forest is that there are not enough y-values with output "yes" to get our model to converge on anything but only outputting "no" for every instance.

We also performed a simple logistic regression, with a similar result:

```{r, echo = FALSE, warning = FALSE}
lr_model <- glm(y ~ ., data = train, family = binomial)
prob = predict(lr_model, test)
pred = ifelse(prob > 0.5, 1, 0)
cat("Accuracy:", mean(pred == test$y), "\n")
table(pred, test$y)
```

Here at least the model was able to predict a couple of "yes" values, however they were incorrectly calculated once again since the test set only contained "no"s. 

The overall weakness with these models is not only a result of not having enough samples, but also with their optimizaiton. Both random forest and logistic regression optimize for accuracy in prediction. While that is important, in the context of this problem we would rather have false positives than false negatives, meaning we would rather assume someone would sign up when they wouldn't and waste a call than assume they wouldn't sign up when they would have if they had just been contacted. 

As such, we utilized a more intensive boosting model that utilizes the aformentioned shrinkage parameter in order to make it such that the model can more accurately converge to predicting "yes" values.

```{r, echo = FALSE}
set.seed(1)
n=nrow(bank_data)
ntree=1500 #Number of Trees
tsize=5000 #size of test set
sam=sample(n,tsize)
test=bank_data[sam,]
train=bank_data[-sam,]
```

```{r, echo = FALSE}
# This evaluation function implement cross validation manually.
eval<-function(shrink){
  sam2=sample(n-tsize,n-tsize)
  score=0
  pr=0
  for (i in 0:4){
    kfold<<-sam2[(((n-tsize)/5)*i+1):min(((n-tsize)/5)*(i+1),(n-tsize))]
    tr<<-train[-kfold,]
    val<<-train[kfold,]
    tr=ovun.sample(y~.,tr,method="under",p = .5)$data
    boost=gbm(y~.,data=tr,distribution="bernoulli",n.trees=ntree, shrinkage=shrink)
    pred=predict(boost,val,type="response")
    roc=roc(val$y,pred)$auc
    pred=as.numeric(ifelse(pred>.5,1,0))
    pr=pr+mean(pred==val$y)/5
    roc=roc(val$y,pred)$auc
    score=score+roc/5
  }
  return(score)
}

```


```{r, echo = FALSE}
# Our kernel function for the Bayesian Optimization.
kernel=function(X1, X2, l=1.0, sigma_f=1.0){
    sqdist =  sweep(sweep(- 2 * X1%*% t(X2),1,rowSums(X1^2),FUN="+"),2, rowSums(X2^2),FUN="+")
    return (sigma_f^2 * exp(-0.5 / l^2 * sqdist))
}

```


```{r, echo = FALSE}
# Posterior function utilizes the above Kernel function to make predictions about the mean and variance at the points X_s. 
posterior=function(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8){
    K = kernel(X_train, X_train, l, sigma_f) + sigma_y^2 * diag(rep(1,nrow(X_train)))
    K_s = kernel(X_train, X_s, l, sigma_f)
    K_ss = kernel(X_s, X_s, l, sigma_f)+ 1e-8 * diag(rep(1,nrow(X_s)))
    K_inv = pinv(K)
    mu_s = t(K_s)%*%K_inv%*%Y_train
    cov_s = K_ss - t(K_s)%*%K_inv%*%K_s
    return (list(mu_s, cov_s))}

```

```{r, echo = FALSE}
# Gives us the point to sample next via the expected improvement criterium.
expected_improvement=function( x_proposed,X_train, Y_train,l=1.0, sigma_f=1.0, sigma_y=.001){
    out = posterior(matrix(x_proposed,ncol=1), X_train, Y_train, l, sigma_f, sigma_y)
    mu=out[[1]]
    var=diag(out[[2]])
    y_current=max(Y_train)
    std = sqrt( var)
    delta = mu - y_current
    std[std == 0] = Inf
    z = delta / std
    return (delta * pnorm(z) + std * dnorm(z))
}

```

First we set up our intial sampled points equaly spread out, and then afterwards chooses 10 more points based on the Bayesian Optimization algorithm. For this we used a log scale.
```{r,message=FALSE, echo = FALSE}
vals <- seq(-4, 0, length.out = 5)
scores=c()
for (i in 1:5){
  scores=c(scores,eval(10^(vals[i])))
}
for (i in 1:11){
x_new <- seq(-4, 0, length.out = 1000)
pred <- expected_improvement(x_new,matrix(vals),matrix(scores),l=.5,sigma_f=.5,sigma_y = .01)
a= x_new[which.max(log10(pred))]
score=eval(10^a)
vals=c(vals,a)
scores=c(scores,score)
}
```

We then create a "Visualization" of the Gaussian Process and Expected Improvement. The left graph shows the estimate of our function and the right graph is the Expected Improvment function which we used to choose the next point to sample. The vertical line represents the shrinkage of the next point we will sample.

```{r, echo = FALSE}
start=4
for (i in 1:11){
x_new <- seq(-4, 0, length.out = 1000)
pred <- expected_improvement(x_new,matrix(vals[1:(start+i)]),matrix(scores[1:(start+i)]),l=.5,sigma_f=.5,sigma_y = .01)
data = data.frame(x = x_new, y = log10(pred))
colnames(data)=c("x","y")
line=x_new[which.max(pred)]
assign("p2",ggplot(data,mapping=aes(x = x, y = y))+
  geom_line(color = "red", linetype = "dashed")+
  theme_minimal()+geom_vline(xintercept=line))+
  ylab("Expected Improvment")+xlab("log(shrinkage)")

x_new <- seq(-4, 0, length.out = 1000)
pred <- posterior(matrix(x_new),as.matrix(vals[1:(start+i)]),matrix(scores[1:(start+i)]),l=1,sigma_f=1,sigma_y = .001)
mu <- pred[[1]]
sigma <- sqrt(diag(pred[[2]]))
data = data.frame(x = x_new, y = mu,y_up = mu + sigma, y_low = mu - sigma)
data2=data.frame(x=vals[1:(start+i)],y=matrix(scores[1:(start+i)]))
colnames(data)=c("x","y","y_up","y_low")
assign("p1",ggplot()+
  geom_line(data=data,color = "red", linetype = "dashed",mapping=aes(x = x, y = y))+
  geom_ribbon(data=data,fill = "skyblue", alpha = 0.5,mapping=aes(x = x, y = y,ymax = y_up, ymin = y_low)) +
  theme_minimal()+
  geom_point(data=data2,mapping=aes(x = x, y = y))+geom_vline(xintercept=line))+
  ylab("AUC")+xlab("log(shrinkage)")
grid.arrange(p1,p2,ncol=2)
}
```

We choose a shrinkage value based off of Gaussian Process.
```{r, echo = FALSE}
x_new <- seq(-4, 0, length.out = 1000)
pred <- posterior(matrix(x_new),as.matrix(vals),matrix(scores),l=1,sigma_f=1,sigma_y = .001)
mu <- pred[[1]]
(shrink=10^x_new[which.max(mu)])
```

Now we oversample the training set, and perform boosting. The following graph is a summary of the boosting and a relative influence graph.
```{r, echo = FALSE}
train=ovun.sample(y~.,train,method="under",p=.5)$data
boost=gbm(y~.,data=train,distribution="bernoulli",n.trees=ntree, shrinkage=shrink)
summary(boost)
```
From looking at the important table and graph, the most important predictor was the persons "job," which makes sense logically since wealthier people will likely have more money to invest. It is interesting though that this was such an important predictor though given that there did not appear to be a strong correlation with sucess in the boxplot of jobs. Euribor3m is a close second. As we saw above, it should be noted that this variable was strongly correlaated with the "employment variation rate" and "number of employees." All of these are metrics of the economy and are likely also correlated with the interest rates the bank is offering. "Month" was a distant but clear third; this is of interest as nothing immediately comes to mind for why month would have a strong effect. One possibility is that given that the data was gathered over the short span of a year and a half, outside events could have had an effect on some months but there was not enough time for those outside unrelated events to get averaged out. Following month, no other variables make larger contributions, although they do have small effects.


Evaluating via AUC, accuracy, and table gives us the following.
```{r, echo = FALSE}
pred=predict(boost,test,type="response")
roc=roc(test$y,pred,plot=TRUE)
cat("AUC:",roc$auc,"\n")
pred=ifelse(predict(boost,test,type="response")>.5,1,0)
cat("Accuracy:",mean(pred==test$y),"\n")
cat("Contingency table:\n")
table(pred,test$y)
```
Our model's AUC of .8029 is commendable, given that week exluded the "duration of call variable." Had we included that variable, our model's AUC increased to .94 (not shown), whichis very good. The accuracy is only ~81%, but that is not our metric of interest. If we look at the contigency table, most of our errors resulted from False positives, the prefered error since we'd rather make an extra call than lose a customer. As the bank, there is more of a cost to not calling a customer when they would have said yes than there is to calling a customer who says no.

```{r, echo = FALSE}
# Question 2

# process data repeats the above data processing without printing visuals
process_data = function(bank_data) {
  bank_data[bank_data=="unknown"]=NA


  bank_data$y=ifelse(bank_data$y=="yes",1,0)
  
  
  bank_data = within(bank_data,rm("duration"))
  
  
  bank_data$education=ifelse(bank_data$education=="illiterate",0,ifelse(bank_data$education=="basic.4y",1,ifelse(bank_data$education== "basic.6y",2,ifelse(bank_data$education== "basic.9y",3,ifelse(bank_data$education== "high.school",4,ifelse(bank_data$education== "university.degree",5,ifelse(bank_data$education== "professional.course",0,NA)))))))
  bank_data$education=as.numeric(bank_data$education)
  mean=mean(bank_data$education,na.rm=TRUE) 
  bank_data$education[which(is.na(bank_data$education))]=mean
  
  
  bank_data$housing[which(is.na(bank_data$housing))]="unknown"
  
  
  bank_data=within(bank_data,rm("default"))
  
  bank_data=within(bank_data,rm("nr.employed"))
  bank_data=within(bank_data,rm("emp.var.rate"))
  
  
  bank_data$day_of_week=ifelse(bank_data$day_of_week=="mon",0,ifelse(bank_data$day_of_week=="tue",1,ifelse(bank_data$day_of_week=="wed",2,ifelse(bank_data$day_of_week=="thu",3,ifelse(bank_data$day_of_week== "fri",4,NA)))))
  bank_data$day_of_week=as.numeric(bank_data$day_of_week)
  bank_data$day_of_week=as.numeric(bank_data$day_of_week)
  mean=mean(bank_data$day_of_week,na.rm=TRUE)
  bank_data$day_of_week[which(is.na(bank_data$day_of_week))]=mean
  
  bank_data$housing[which(is.na(bank_data$housing))]="unknown"
  
  
  bank_data$loan[which(is.na(bank_data$loan))]="unknown"
  
  
  bank_data$marital=impute(bank_data$marital,fun=mode)
  
  
  bank_data=within(bank_data,rm("poutcome"))
  
  
  bank_data$month[which(bank_data$month=="nov" | bank_data$month=="dec")]="nov/dev"
  
  bank_data$month=as.factor(bank_data$month)
  
  
  bank_data$previous[which(bank_data$previous>=5)]=5
  
  return(bank_data)
}
```

```{r, echo = FALSE}
# upsample data repeats the above code for boosting the data without printing any visuals
upsample_data = function(bank_data) {
  ntree <<- 1500 #Number of Trees
  tsize <<- 5000 #size of test set
  sam <<- sample(n,tsize)
  test <<- bank_data[sam,]
  train <<- bank_data[-sam,]
  vals <- seq(-4, 0, length.out = 5)
  scores=c()
  for (i in 1:5){
    scores=c(scores,eval(10^(vals[i])))
  }
  for (i in 1:11){
    x_new <- seq(-4, 0, length.out = 1000)
    pred <- expected_improvement(x_new,matrix(vals),matrix(scores),l=.5,sigma_f=.5,sigma_y = .01)
    a= x_new[which.max(log10(pred))]
    score=eval(10^a)
    vals=c(vals,a)
    scores=c(scores,score)
  }
  x_new <- seq(-4, 0, length.out = 1000)
  pred <- posterior(matrix(x_new),as.matrix(vals),matrix(scores),l=1,sigma_f=1,sigma_y = .001)
  mu <- pred[[1]]
  (shrink=10^x_new[which.max(mu)])
  train=ovun.sample(y~.,train,method="under",p=.5)$data
  boost=gbm(y~.,data=train,distribution="bernoulli",n.trees=ntree, shrinkage=shrink)
  return(list(train, test, boost))
}
```


### Question 2: Is having the additional data (economic data) necessary for a well performing model?

Read in the data and process we did above (using a function we create which does the same thing as what we did above).
```{r, message = FALSE, echo = FALSE}
bank_data = read_delim("bank-additional/bank-additional-full.csv",";",col_names=TRUE,col_types = "dfffffffcfddddfdddddf")
bank_data = process_data(bank_data) #performs the processing as outline in part 1
```

Remove the economic predictors.
```{r, echo = FALSE}
bank_data=within(bank_data,rm("cons.price.idx"))
bank_data=within(bank_data,rm("cons.conf.idx"))
bank_data=within(bank_data,rm("euribor3m"))
```

Using our functions, we will perform Bayesian Optimization to find the optimal shrinkage parameter, and we will then perform model training on the train set. 
```{r,message=FALSE, echo = FALSE}
set.seed(1)
n = nrow(bank_data)
upsample_output = upsample_data(bank_data)

train = upsample_output[[1]]
test = upsample_output[[2]]
boost = upsample_output[[3]]
```

Look at the importance Variables in the model.
```{r, echo = FALSE}
summary(boost)
```
Interesting, the model now relies heavily on "pdays," "month," and "contact." Only month was in the top 3 importance of the previous model. "Job" no longer plays a very important role. "Pdays" seems to be a surprising predictor, but if we recall from preprocessing, it was heavily correlated with "poutcome," which could be the driving factor in this. Method of contact could have some merit since it's possible that a person having a telephone tells is a predictor of other stuff about the person.

Use the model to make predictions and then evaluate the model.
```{r,message=FALSE, echo = FALSE}
pred=predict(boost,test,type="response")s
roc=roc(test$y,pred,plot=FALSE)
cat("AUC:",roc$auc,"\n")
pred=ifelse(predict(boost,test,type="response")>.5,1,0)
cat("Accuracy:",mean(pred==test$y),"\n")
cat("Contingency table:\n")
table(pred,test$y)
```
The model's predictions as only slightly worse (~.04 AUC) than with the economic variables. This is a surprising result given that the previous model relied heavily on the economic variables. If we include the duration variable it has an AUC of ~.87. So while the economic factors do help, they aren't all that necessary. 


### Question 3: Does excluding NA and unknown values from the model at the beginning have a significant effect on the model?

```{r, message = FALSE, echo = FALSE}
# Question 3

bank_data = read_delim("bank-additional/bank-additional-full.csv",";",col_names=TRUE,col_types = "dfffffffcfddddfdddddf")

bank_data[bank_data=="unknown"] = NA
bank_data = na.exclude(bank_data)

```

Removing 'unknown' and 'NA' values reduces the data from 41188 observations to 30488 observations. While this is a large difference, it is still a significantly large number of observations so it does hurt the model since it will be training on less data.

```{r, message = FALSE, echo = FALSE}
bank_data = process_data(bank_data)
n = nrow(bank_data)
upsample_output = upsample_data(bank_data)

train = upsample_output[[1]]
test = upsample_output[[2]]
boost = upsample_output[[3]]

pred=predict(boost,test,type="response")
roc=roc(test$y,pred,plot=FALSE)
cat("AUC:",roc$auc,"\n")
pred=ifelse(predict(boost,test,type="response")>.5,1,0)
cat("Accuracy:",mean(pred==test$y),"\n")
cat("Contingency table:\n")
table(pred,test$y)
```

As shown by the AUC, Accuracy, and Contingency table, the results of excluding NA values from the dataset has no significant difference from the original model. This is understandable from the way that we set up our problem, and just confirms that having an unknown value in any attribute is not a predictor for whether or not the subject will sign up for a bank deposit.

 
# References  

1. Friedman, Jerome H. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics, vol. 29, no. 5, 2001, pp. 1189–1232. JSTOR, www.jstor.org/stable/2699986. Accessed 12 Dec. 2020.  
2. Chen, Tianqi, and Tong He. “Xgboost: EXtreme Gradient Boosting.” 2 Sept. 2020.    
3. Snoek, Jasper, et al. “Practical Bayesian Optimization of Machine Learning Algorithms.” 29 Aug. 2012. 

### R Appendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```